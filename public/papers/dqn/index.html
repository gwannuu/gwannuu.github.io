<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>DQN | Gwannuu blog</title>
<meta name="keywords" content="">
<meta name="description" content="In this paper[@mnih2013playing], atari game is solved with combination of CNN network and Q-Learning algorithm.
Q Learning
Before talking about DQN, first see about Q-Learning.
The objective of Q-Learning is to find optimal action-state $$ Q^\ast (s,a) = \max_{\pi} \mathbb{E_{\pi}[r\mid s, a]} $$
And in each step, learned agent selects greedy based on its learned action value function.
In other words, in state $s \in \mathcal{S}$, agent select action $$a = \arg\max_{a^\prime \in \mathcal{A}(s)}Q(s, a^\prime) $$.">
<meta name="author" content="">
<link rel="canonical" href="https://gwannuu.github.io/papers/dqn/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css" integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF&#43;13Dyqob6ASlTrTye8=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://gwannuu.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://gwannuu.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://gwannuu.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://gwannuu.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://gwannuu.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://gwannuu.github.io/papers/dqn/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']],                  
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>

<meta property="og:url" content="https://gwannuu.github.io/papers/dqn/">
  <meta property="og:site_name" content="Gwannuu blog">
  <meta property="og:title" content="DQN">
  <meta property="og:description" content="In this paper[@mnih2013playing], atari game is solved with combination of CNN network and Q-Learning algorithm.
Q Learning Before talking about DQN, first see about Q-Learning.
The objective of Q-Learning is to find optimal action-state $$ Q^\ast (s,a) = \max_{\pi} \mathbb{E_{\pi}[r\mid s, a]} $$ And in each step, learned agent selects greedy based on its learned action value function. In other words, in state $s \in \mathcal{S}$, agent select action $$a = \arg\max_{a^\prime \in \mathcal{A}(s)}Q(s, a^\prime) $$.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="papers">
    <meta property="article:published_time" content="2025-03-14T14:28:57+09:00">
    <meta property="article:modified_time" content="2025-03-14T14:28:57+09:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DQN">
<meta name="twitter:description" content="In this paper[@mnih2013playing], atari game is solved with combination of CNN network and Q-Learning algorithm.
Q Learning
Before talking about DQN, first see about Q-Learning.
The objective of Q-Learning is to find optimal action-state $$ Q^\ast (s,a) = \max_{\pi} \mathbb{E_{\pi}[r\mid s, a]} $$
And in each step, learned agent selects greedy based on its learned action value function.
In other words, in state $s \in \mathcal{S}$, agent select action $$a = \arg\max_{a^\prime \in \mathcal{A}(s)}Q(s, a^\prime) $$.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Papers",
      "item": "https://gwannuu.github.io/papers/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "DQN",
      "item": "https://gwannuu.github.io/papers/dqn/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DQN",
  "name": "DQN",
  "description": "In this paper[@mnih2013playing], atari game is solved with combination of CNN network and Q-Learning algorithm.\nQ Learning Before talking about DQN, first see about Q-Learning.\nThe objective of Q-Learning is to find optimal action-state $$ Q^\\ast (s,a) = \\max_{\\pi} \\mathbb{E_{\\pi}[r\\mid s, a]} $$ And in each step, learned agent selects greedy based on its learned action value function. In other words, in state $s \\in \\mathcal{S}$, agent select action $$a = \\arg\\max_{a^\\prime \\in \\mathcal{A}(s)}Q(s, a^\\prime) $$.\n",
  "keywords": [
    
  ],
  "articleBody": "In this paper[@mnih2013playing], atari game is solved with combination of CNN network and Q-Learning algorithm.\nQ Learning Before talking about DQN, first see about Q-Learning.\nThe objective of Q-Learning is to find optimal action-state $$ Q^\\ast (s,a) = \\max_{\\pi} \\mathbb{E_{\\pi}[r\\mid s, a]} $$ And in each step, learned agent selects greedy based on its learned action value function. In other words, in state $s \\in \\mathcal{S}$, agent select action $$a = \\arg\\max_{a^\\prime \\in \\mathcal{A}(s)}Q(s, a^\\prime) $$.\nAgent behaviors by greedy policy for well trained action value function $ Q(s,a) $ in model-free environment.\nDeep Q Learning In Deep Q learning, parametrized action value function $ Q(s, a) $ as neural network is trained. It is denoted by $Q_\\theta (s, a)$. By applying bellman equation $$\\mathbb{E}[r + \\gamma \\max_{a^\\prime \\in \\mathcal{A}} Q^\\ast (s^\\prime, a^\\prime ) \\mid s ,a] = Q^\\ast (s,a)$$ as an iterative update $$ Q_{i+1} \\gets \\mathbb{E}[r + \\gamma \\max_{a^\\prime \\in \\mathcal{A}} Q_{i}(s^\\prime,a^\\prime )\\mid s,a] $$ this approximates for optimal action value function \\( Q^\\ast(s,a) \\) \\( (Q_i \\rightarrow Q^\\ast \\) ,as \\( i \\rightarrow \\infty) \\) .\nThe Q-network is referred to as a neural network function approximator with weights $\\theta$ ($Q_\\theta (s,a) \\approx Q(s,a)$) Then loss function for each iteration can be written by $$ \\begin{gathered} L_i (\\theta_i) = \\mathbb{E}_{s,a \\sim \\rho(\\cdot)} [(y_i - Q(s,a ;\\theta))^2] \\\\ y_i = \\mathbb{E}_{s^\\prime \\sim \\mathcal{S}} [r + \\gamma \\max_{a^\\prime} Q(s^\\prime, a^\\prime; \\theta_{i-1}) \\mid s,a] \\end{gathered} $$\nwhere $ \\rho(s,a) $ is a probability distribution over sequences $s$ and actions $a$ that is referred to as the behaviour distribution. Usually $\\epsilon$-greedy becomes the behaviour policy and greedy strategy becomes target policy in DQN algorithm. So DQN is off-policy learning, in which behavior policy and target policy that we want to know is different.\nExperience replay In this paper, experience replay technique is utilized, where the agent’s experiences at each time step $e_t = (s_t, a_t, r_t, s_{t+1})$ is stored in dataset $\\mathcal{D} = \\{e_1, \\dots, e_N\\}$. In each update, samples experiences in experience buffer $\\mathcal{D}$. There are some advantages of experience replay.\nThis technique reduces temporal correlation, which refers to the situation where an agent catastrophically forgets long-past experiences from the current time step. learning directly from consecutive samples is inefficient, due to strong correlations between samples Each experience is repeatedly sampled for update, which leads to data efficiency DQN with experience replay algorithm In this algorithm, assume that agent gets preprocessed $\\phi(s)$ instead of raw state $s$.\nInitialize replay memory $\\mathcal{D}$ to capacity $N$\nInitialize action-value function $Q$ with random weights\nFor episode = 1, M do\nInitialize sequence $s_1 = {x_1}$ and preprocessed sequence $\\phi_1 = \\phi(s_1)$\nFor $t = 1, T$ do\nWith probability $\\epsilon$, select a random action $a_t$ Otherwise, select\n$$ a_t = \\arg\\max_a Q^\\ast(\\phi(s_t), a; \\theta) $$ Execute action $a_t$ in the emulator and observe reward $r_t$ and image $x_{t+1}$ Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$ Store transition $(\\phi_t, a_t, r_t, \\phi_{t+1})$ in $\\mathcal{D}$ Sample a random minibatch of transitions $(\\phi_j, a_j, r_j, \\phi_{j+1})$ from $\\mathcal{D}$ Compute target value: $$ y_j = \\begin{cases} r_j, \u0026 \\text{for terminal } \\phi_{j+1} \\\\ r_j + \\gamma \\max_{a^\\prime} Q(\\phi_{j+1}, a^\\prime; \\theta), \u0026 \\text{for non-terminal } \\phi_{j+1} \\end{cases} $$ Perform a gradient descent step on $(y_j - Q(\\phi_j, a_j; \\theta))^2$ End for\nEnd for\nDQN with atari There already exists Q-learning algorithms that has been combined with experience replay and a simple neural network.[13] But it starts with a low-dimensional state rather than raw visual inputs, which is in high-diemnsion.\nPreprocessing Raw atari frame has $210 \\times 160$ pixel images with a 128 color palette, which is in high dimension. In this paper, below process is used as preprocess procedure $\\phi$.\nFirst convert RGB color channel to gray color scale and down sample to $110 \\times 84$. And crop the image to roughly capture the playing area, resulting in an $84 \\times $84 pixel image. Finally, by stacking last 4 frames, can obtain $84 \\times 84 \\times 4$ size of image. Model architecture First cnn layer: $16$ number of $8 \\times 8$ kernel with stride $4$ following by ReLU. Second cnn layer: $32$ number of $4 \\times 4$ kernel with stride $2$ following by ReLU. fully connected layer: consists of $256$ outputs following by ReLU. outpyt layer: consists of number of action space $|\\mathcal{A}|$. Experiments In this paper seven atari games - Beam Rider, Breakout, Enduro, Pong, $Q\\ast$bert, Seaquest, Space Invaders - are trained with same network architecture and hyperparameters. Without leveraging game specific information, DQN algorithm operates robustly.\nReward clipping One special point is reward clipping, which convert reward to one of $ \\{-1, 0, 1\\}$ by the sign of it. Clipping the rewards limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games.\nFrame skipping frame skipping technique, in which agent sees and selects action on every $k$th frame instead of every frame, and its last action is repeated on skipped frames.\nNoteworthy points In Q Learning, Batch normalization is not used. Batch normalization normalizes channels of batched features. But in DQN algorithm to solve breakout reflects time relation to channels of features.\nAlso, it does not utilize max pool. Original input images has size of $(84, 84)$, which have very small resolution. If utilize maxpool, then important pixel information can be disappeared.\nInstead of using max pooling layer, in this paper, they utilizes stride convolution layer so that reduces sizes of feature map.\n",
  "wordCount" : "911",
  "inLanguage": "en",
  "datePublished": "2025-03-14T14:28:57+09:00",
  "dateModified": "2025-03-14T14:28:57+09:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://gwannuu.github.io/papers/dqn/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Gwannuu blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://gwannuu.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://gwannuu.github.io/" accesskey="h" title="Gwannuu blog (Alt + H)">Gwannuu blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://gwannuu.github.io/articles/" title="Articles">
                    <span>Articles</span>
                </a>
            </li>
            <li>
                <a href="https://gwannuu.github.io/papers/" title="Papers">
                    <span>Papers</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      DQN
    </h1>
    <div class="post-meta"><span title='2025-03-14 14:28:57 +0900 KST'>March 14, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>In this paper[@mnih2013playing], atari game is solved with combination of CNN network and Q-Learning algorithm.</p>
<h1 id="q-learning">Q Learning<a hidden class="anchor" aria-hidden="true" href="#q-learning">#</a></h1>
<p>Before talking about DQN, first see about Q-Learning.</p>
<p>The objective of Q-Learning is to find optimal action-state $$ Q^\ast (s,a) = \max_{\pi} \mathbb{E_{\pi}[r\mid s, a]} $$
And in each step, learned agent selects greedy based on its learned action value function.
In other words, in state $s \in \mathcal{S}$, agent select action $$a = \arg\max_{a^\prime \in \mathcal{A}(s)}Q(s, a^\prime) $$.</p>
<p>Agent behaviors by greedy policy for well trained action value function $ Q(s,a) $ in model-free environment.</p>
<h1 id="deep-q-learning">Deep Q Learning<a hidden class="anchor" aria-hidden="true" href="#deep-q-learning">#</a></h1>
<p>In Deep Q learning, parametrized action value function $ Q(s, a) $ as neural network is trained. It is denoted by $Q_\theta (s, a)$. By applying bellman equation $$\mathbb{E}[r + \gamma \max_{a^\prime \in \mathcal{A}} Q^\ast (s^\prime, a^\prime ) \mid s ,a] = Q^\ast (s,a)$$ as an iterative update $$ Q_{i+1} \gets \mathbb{E}[r + \gamma \max_{a^\prime \in \mathcal{A}} Q_{i}(s^\prime,a^\prime )\mid s,a] $$ this approximates for optimal action value function \( Q^\ast(s,a) \)  \( (Q_i \rightarrow Q^\ast \) ,as  \( i \rightarrow \infty) \) .</p>
<p>The Q-network is referred to as a neural network function approximator with weights $\theta$ ($Q_\theta (s,a) \approx Q(s,a)$)
Then loss function for each iteration can be written by
$$
\begin{gathered}
L_i (\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} [(y_i - Q(s,a ;\theta))^2] \\
y_i = \mathbb{E}_{s^\prime \sim \mathcal{S}} [r + \gamma \max_{a^\prime} Q(s^\prime, a^\prime; \theta_{i-1}) \mid s,a]
\end{gathered}
$$</p>
<p>where $ \rho(s,a) $ is a probability distribution over sequences $s$ and actions $a$ that is referred to as the behaviour distribution. Usually $\epsilon$-greedy becomes the behaviour policy and greedy strategy becomes target policy in DQN algorithm. So DQN is off-policy learning, in which behavior policy and target policy that we want to know is different.</p>
<h2 id="experience-replay">Experience replay<a hidden class="anchor" aria-hidden="true" href="#experience-replay">#</a></h2>
<p>In this paper, <strong>experience replay</strong> technique is utilized, where the agent&rsquo;s experiences at each time step $e_t = (s_t, a_t, r_t, s_{t+1})$ is stored in dataset $\mathcal{D} = \{e_1, \dots, e_N\}$. In each update, samples experiences in experience buffer $\mathcal{D}$.
There are some advantages of experience replay.</p>
<ul>
<li>This technique reduces temporal correlation, which refers to the situation where an agent catastrophically forgets long-past experiences from the current time step.
<ul>
<li>learning directly from consecutive samples is inefficient, due to strong correlations between samples</li>
</ul>
</li>
<li>Each experience is repeatedly sampled for update, which leads to data efficiency</li>
</ul>
<h2 id="dqn-with-experience-replay-algorithm">DQN with experience replay algorithm<a hidden class="anchor" aria-hidden="true" href="#dqn-with-experience-replay-algorithm">#</a></h2>
<p>In this algorithm, assume that agent gets preprocessed $\phi(s)$ instead of raw state $s$.</p>
<ul>
<li>
<p>Initialize replay memory $\mathcal{D}$ to capacity $N$</p>
</li>
<li>
<p>Initialize action-value function $Q$ with random weights</p>
</li>
<li>
<p><strong>For episode = 1, M do</strong></p>
<ul>
<li>
<p>Initialize sequence $s_1 = {x_1}$ and preprocessed sequence $\phi_1 = \phi(s_1)$</p>
</li>
<li>
<p><strong>For $t = 1, T$ do</strong></p>
<ul>
<li>With probability $\epsilon$, select a random action $a_t$</li>
<li>Otherwise, select<br>
$$
a_t = \arg\max_a Q^\ast(\phi(s_t), a; \theta)
$$
<ul>
<li>Execute action $a_t$ in the emulator and observe reward $r_t$ and image $x_{t+1}$</li>
<li>Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$</li>
<li>Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in $\mathcal{D}$</li>
<li>Sample a random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from $\mathcal{D}$</li>
<li>Compute target value:
$$
y_j =
\begin{cases}
r_j, &amp; \text{for terminal } \phi_{j+1} \\
r_j + \gamma \max_{a^\prime} Q(\phi_{j+1}, a^\prime; \theta), &amp; \text{for non-terminal } \phi_{j+1}
\end{cases}
$$</li>
<li>Perform a gradient descent step on $(y_j - Q(\phi_j, a_j; \theta))^2$</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>End for</strong></p>
</li>
</ul>
</li>
<li>
<p><strong>End for</strong></p>
</li>
</ul>
<h1 id="dqn-with-atari">DQN with atari<a hidden class="anchor" aria-hidden="true" href="#dqn-with-atari">#</a></h1>
<p>There already exists Q-learning algorithms that has been combined with experience replay and a simple neural network.[13] But it starts with a low-dimensional state rather than raw visual inputs, which is in <strong>high-diemnsion</strong>.</p>
<h2 id="preprocessing">Preprocessing<a hidden class="anchor" aria-hidden="true" href="#preprocessing">#</a></h2>
<p>Raw atari frame has $210 \times 160$ pixel images with a 128 color palette, which is in high dimension.
In this paper, below process is used as preprocess procedure $\phi$.</p>
<ul>
<li>First convert RGB color channel to gray color scale and down sample to $110 \times 84$.</li>
<li>And crop the image to roughly capture the playing area, resulting in an $84 \times $84 pixel image.</li>
<li>Finally, by stacking last 4 frames, can obtain $84 \times 84 \times 4$ size of image.</li>
</ul>
<h2 id="model-architecture">Model architecture<a hidden class="anchor" aria-hidden="true" href="#model-architecture">#</a></h2>
<ul>
<li>First cnn layer: $16$ number of $8 \times 8$ kernel with stride $4$ following by ReLU.</li>
<li>Second cnn layer: $32$ number of $4 \times 4$ kernel with stride $2$ following by ReLU.</li>
<li>fully connected layer: consists of $256$ outputs following by ReLU.</li>
<li>outpyt layer: consists of number of action space $|\mathcal{A}|$.</li>
</ul>
<h1 id="experiments">Experiments<a hidden class="anchor" aria-hidden="true" href="#experiments">#</a></h1>
<p>In this paper seven atari games - Beam Rider, Breakout, Enduro, Pong, $Q\ast$bert, Seaquest, Space Invaders - are trained with same network architecture and hyperparameters. Without leveraging game specific information, DQN algorithm operates robustly.</p>
<h2 id="reward-clipping">Reward clipping<a hidden class="anchor" aria-hidden="true" href="#reward-clipping">#</a></h2>
<p>One special point is reward clipping, which convert reward to one of $ \{-1, 0, 1\}$ by the sign of it. Clipping the rewards limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games.</p>
<h2 id="frame-skipping">Frame skipping<a hidden class="anchor" aria-hidden="true" href="#frame-skipping">#</a></h2>
<p>frame skipping technique, in which agent sees and selects action on every $k$th frame instead of every frame, and its last action is repeated on skipped frames.</p>
<h1 id="noteworthy-points">Noteworthy points<a hidden class="anchor" aria-hidden="true" href="#noteworthy-points">#</a></h1>
<p>In Q Learning, Batch normalization is not used. Batch normalization normalizes channels of batched features. But in DQN algorithm to solve breakout reflects time relation to channels of features.</p>
<p>Also, it does not utilize max pool. Original input images has size of $(84, 84)$, which have very small resolution. If utilize maxpool, then important pixel information can be disappeared.</p>
<p>Instead of using max pooling layer, in this paper, they utilizes stride convolution layer so that reduces sizes of feature map.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://gwannuu.github.io/">Gwannuu blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
